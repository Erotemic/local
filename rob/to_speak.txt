\subsection{Encounter-Based Image Analysis Subsystem}The ability to scale up to many thousands of images per day in a smallarea and identify thousands of different animals implies that imageanalysis must be automated as much as possible.  Therefore, thedevelopment and implementation of image analysis / computer visionalgorithms is a large focus of our IBEIS effort.  This level ofautomation is indeed possible, partially because of the power ofalgorithms that have recently appeared in the computer visionliterature (including our own) and in part through exploitingredundancies in images, both from individual encounters and over manyrepeated sightings.While the goal of the computation is the formation and analysis of anencounter, the initial unit of computation is a single image --- todetect animals, to determine their species and, where possible, toidentify them individually.  Each of these becomes a separateannotation.  We add two specializations of this: to determine regionsof an image containing animals of a certain species without separatingthem into individual animals, and to determine when we can know thereis an animal of a certain species, but there is not enough informationto identify the animal individually.  This can either be because theindividuals are not distinguishable or because the viewpoint on theanimal does not provide distinguishing information.In what follows we describe the steps of analyzing individual imagesand then consider how they will be combined them to analyze andencounter.  Perhaps surprisingly, we start with HotSpotter, ourindividual identification algorithm, because we will use it to improvedetection and species determination as well.\paragraph{HotSpotter} In its current version, HotSpotter\cite{crall_hotspotter_2013} works with a pre-specified image regionof interest surrounding one animal.  It (1) extracts affine keypoints\cite{perdoch_efficient_2009} at multiple scales, (2) computes SIFTand SURF descriptors (or the like) \cite{lowe_distinctive_2004,  bay_surf:_2006}, (3) matches these against a spatial database\cite{} of the descriptors images from previously-seen individuals tofind a set of the $k$ nearest neighbors for each descriptor, (4)aggregates descriptor matching on an image-by-image basis using amodified Naive-Bayes nearest neighbor \cite{boiman_defense_2008,  mccann_local_2012} scoring mechanism, (5) filters the matches forspatial consistency, and (6) generates a final ranking for each ofpotential matching animal from the database. Results are presented tothe user, allowing him/her to examine the matches --- the ``hotspots''--- as illustrated in Figure~XX.  HotSpotter follows the general formof ``instance recognition'' algorithms \cite{nister_scalable_2006,  philbin_object_2007, sivic_efficient_2009, chum_total_2011},although we have shown experimentally that our non-vocabulary approachand scoring mechanism outperform methods from the literature.  In thisregard, while the novelty of HotSpotter is modest and largely in thedetails, the performance is outstanding --- over 95\% probability ofproducing the correct animal identity from its first match on plainsand Grevy's zebras, giraffes, leopards, and a number of otherspecies.Our experimental evaluation shows that two innovations are the key toimproving HotSpotter for IBEIS.  (1) An improved scoring mechanism isneeded to better distinguish correct matches, and reduce the need formanual review of results.  (2) A better method of handling viewpointvariations is needed.  While improvements will be made to a number ofthe components of HotSpotter, the most important innovation to addressboth of these is to model each animal as an aggregation of image ROIsfrom a variety of viewpoints --- above, left, right, behind, top,etc., and store best-example images from each viewpoint (when it hasbeen seen).  There can be a temporal component to these so that IBEIScan gradually adjust to changes in appearance.  Viewpoint pose can beroughly obtained by combining the outputs of multiple detectors and byanalyzing matching results.  We will roughly cluster the images fromeach animal, adding new images that are sufficiently different.  Thus,each animal will be represented by a variety of images, and decisionswill be made based on the most similar match.\paragraph{Single image detection: combined with identification}Detection and species determination, which we will refer to simply asdetection, are natural precursors to identification of individualanimals; the most obvious reason of why detection is crucial is that the search space is significantly reduced for determining the individual.In IBEIS, however, because we expect to see particularanimals over and over, both in a single encounter and over manyencounters, individual identification can also help facilitate detection.Our starting point is existing detection algorithms, which can beroughly broken down into sliding window methods\cite{viola_robust_2004, dalal_histograms_2005,  desai_discriminative_2011} and associated parts models\cite{bourdev_poselets:_2009, felzenszwalb_object_2010,  endres_learning_2013}, vocabulary-based / bag-of-words-based methods using multipleclassifiers \cite{vedaldi_multiple_2009} (OTHERS?), Hough-based\cite{leibe_robust_2008} and random forest methods\cite{gall_hough_2011, barinova_detection_2012}, segmentation-basedalgorithms \cite{van_de_sande_segmentation_2011, leibe_robust_2008},exemplar-based methods \cite{malisiewicz_ensemble_2011} and,importantly, recent hybrid methods \cite{}.  We are in the process ofexperimentally evaluating which of these algorithms, which have been largelydeveloped for pedestrian and car detection, best adapts to the task of animal detection.  Figure~XX shows examples of detecting plainszebras and giraffes using of random forest method.  Challenges includea wide range of viewpoints and poses (related works deal with viewpoints by using random forest techniques \cite{hara_k-ary_2013}), and substantialocclusions from vegetation and other animals.  Interestingly,\cite{hoiem_diagnosing_2012} shows that exact localization is a moresubstantial problem than complete mistakes for occlusions.When occlusion and viewpoint lead to poor detection, it will can oftenbe possible to use identification to complete the detection.  Basicdetection methods can still determine regions where animals of aparticular species might appear (i.e., due to seeing a very small, yet highly distinctive portion of the animal), even if the boundaries on theindividual may be unclear.  We will use keypoint matching againstthe HotSpotter database to predict the location and species of anindividually-known animal.  This becomes the predicted detectionregion.  Completing the HotSpotter matching over this region can thenproduce an individual identity, and the eventual detection.  Note thatjust as improvements to HotSpotter, redundancy and repeated viewswill be exploited to substantially improve overall success and, as a directresult, system autonomy.Two final notes on our single image detection/identification systemare important.  First, we will develop a measure of``identifiability'', where descriptors in an animal region arecompared against a database to determine the distinctiveness of theirdescriptors.  Animals showing only non-distinct regions should not beconsidered matching failures or new animals.  Second, detection willalso be extended to camera trap images, using a combination ofbackground modeling algorithms (HENDRIK, CITATIONS HERE) andforeground, species-specific detection algorithms.  Below we discussan addition step of using matching between multiple images in anencounter to further improve detection.\paragraph{Encounters}  These will be generated by a combination ofspatial, temporal, and image-content clustering.  Detection andidentification methods, as just discussed, can be used in the processof forming encounters, while redundancy of images in many encounterscan be used to improve detection and identification results.(As a brief preliminary note, we will also need to add filters toremove low quality images and images that might have been accidentallyuploaded and should not be contributed to the system.  As a test ofthe latter, we implemented an algorithm based on a GIST\cite{oliva2001modeling} with multiple ``foreground'' clusters.  Whentrained and then tested on images from Mpala and from a randomlysampled set of Flickr images, XX\% of the Flickr images wereeliminated without losing any Mpala images.)The initial criteria for linking images in an encounter is proximityin time and location.  Since we assume the images have been collectedtogether, simple methods can be used to synchronize times for multiplecameras (from a single vehicle) and add GPS to images that do not haveit (e.g.\ from other cameras that do or from recorded GPS on avehicle).\footnote{While it may be possible in isolated cases to  automatically recognize at least some locations, we will not  consider these here.}  When considering a habitat encounter,proximity in location is sufficient to link images and form anencounter.  For encounters with an identifiable group of animals, weuse the multiplicity of images together with image analysis algorithmsto form the encounter linkages, to improve algorithm outcomes ascompared to single-image analysis, and to aggregate the results todetermine the complete (or nearly complete) set of animals that appearin an encounter.We do this by building a separate HotSpotter database for eachencounter, and applying graph techniques to resolve ambiguities.  Thelocal HotSpotter database will start with each image and eachannotation region within each image as extracted through single imageanalysis. (Recall that some regions will be single animals and otherswill cover an overlapping collection of animals.)  Confidentidentifications of animals against the ``global'' HotSpotter databasewill be used to initialize known id labels. For animal image regionsthat are unmatched globally, and perhaps even incompletely detected,HotSpotter will be applied using only the local database.  This canpropagate both detection regions, as described above, and ids.Regions will be linked between images based on successfully matches.Graph coverage techniques will determine the final consistency and setof labels, as well as determining what is ambiguous and may need helpfrom a (citizen) scientist.The overall result will be new annotations, propagated labels, and linkagesbetween images.  These will be clustered into encounters.  Theclustering criteria will depend on the desired granularity over timeand space, as well as the ecological unit of interest, all of whichwill be tunable in IBEIS.This encounter-based matching scheme, including the local HotSpotterdatabase, is already being used semi-automatically based on thecurrent version of HotSpotter by technicians on the zebra surveys atMpala!  Figure~xx shows an example of this where a single zebra ismatched from many views, producing the information necessary to buildup the complement of views described above for next generation ofHotSpotter. This does not claim that all known animals in all images in anencounter will be uniquely identified.  In some encounters, especiallyshort ones, an animal may stay largely occluded throughout.  Thisanimal will likely show up as detected but unmatched.  We can useknowledge of previous encounters to know from its associates in theherd who this might be and use a restricted form of matching, similarto local HotSpotter, together with a covering set of views, todetermine who the animal is.This all presumes that the animal is individually identifiable, butthese species are the focus of version 1.0.  Other species, such asimpala and various types of gazelles, are less distinctive.  Thesewill be largely ignored in version 1.0.Finally, when the analysis of an encounter is complete, the resultsare communicated to WildBook.  In many cases this will be repeated,confident identification of already known animals.  In few cases,there may be confident identification of previously unseen animals.These will be new to the database, and depending on parameters set byscientists, may either be silently added or trigger a (citizen)scientist review.  Review may be also be triggered in encounters thatshow identity ambiguities despite multiple views.